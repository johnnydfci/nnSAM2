{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42535758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# nnsam2 Post-processing Pipeline (SAM2 â†’ nnU-Net pseudo-labels)\n",
    "# --------------------------------------------------------------\n",
    "# 1. Merge IOU and mapping results\n",
    "# 2. Select best pid_folder per (pid, frame_idx) using Dice\n",
    "# 3. Copy masks to canonical pid folder\n",
    "# 4. Reconstruct NIfTI from per-slice PNGs\n",
    "# 5. Evaluate Dice similarity vs ground truth\n",
    "#\n",
    "# Supports multiple refinement stages via REFINE_PROFILES\n",
    "# ==============================================================\n",
    "\n",
    "import os, glob, shutil, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import SimpleITK as sitk\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from scipy.ndimage import label\n",
    "\n",
    "# ==============================================================\n",
    "# Configuration: Dataset & Stage Profiles\n",
    "# ==============================================================\n",
    "\n",
    "DATA_PROFILES = {\n",
    "    \"CT_word\": {\n",
    "        \"base_dir\": \"videos/CT_word\",\n",
    "        \"label_dir_2c\": \"data_in_nii/label_in_nii_L4L5_2class\",\n",
    "    },\n",
    "    \"CT_TT\": {\n",
    "        \"base_dir\": \"videos/CT_TT\",\n",
    "        \"label_dir_2c\": \"data_in_nii/label_in_nii_L4L5_2class\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Refinement step parameters (Stage 1 â†’ Stage 3)\n",
    "REFINE_PROFILES = {\n",
    "    \"stage1\": {\n",
    "        \"iou_dataset_top\": 0.10,  # keep top 10% at dataset level\n",
    "        \"iou_slice_top\": 0.02,    # keep top 2% at slice level\n",
    "        \"dice_thresh\": None,      # not applied at Stage 1\n",
    "        \"area_ratio\": None        # not applied at Stage 1\n",
    "    },\n",
    "    \"stage2\": {\n",
    "        \"iou_dataset_top\": 0.10,\n",
    "        \"iou_slice_top\": None,\n",
    "        \"dice_thresh\": 0.90,      # require DSC > 0.90 vs SAM2\n",
    "        \"area_ratio\": 1.5         # smoothness: area â‰¤ 1.5x superior slice\n",
    "    },\n",
    "    \"stage3\": {\n",
    "        \"iou_dataset_top\": 0.20,  # keep top 20%\n",
    "        \"iou_slice_top\": None,\n",
    "        \"dice_thresh\": 0.90,      # require DSC > 0.90 vs Stage1\n",
    "        \"area_ratio\": 1.25        # stricter smoothness\n",
    "    }\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# Choose active dataset & stage\n",
    "# ----------------------------\n",
    "ACTIVE_DATASET = \"CT_word\"\n",
    "ACTIVE_STAGE = \"stage1\"\n",
    "SEED_COUNT = 1\n",
    "SLICE_PROMPT = 1\n",
    "\n",
    "P = DATA_PROFILES[ACTIVE_DATASET]\n",
    "R = REFINE_PROFILES[ACTIVE_STAGE]\n",
    "\n",
    "BASE_DIR = P[\"base_dir\"]\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data_in_jpg_2class\")\n",
    "RESULT_ROOT = os.path.join(DATA_DIR, f\"sam2_results_by_pid_{SEED_COUNT}seed_{SLICE_PROMPT}_slice_prompt\")\n",
    "IMG_MAPPING_ROOT = os.path.join(DATA_DIR, f\"img_in_jpg_to_sam2_{SEED_COUNT}seed\")\n",
    "\n",
    "MERGED_CSV = os.path.join(DATA_DIR, f\"{ACTIVE_DATASET}_2class_IOU_all_pid_with_full_mapping_{SEED_COUNT}seed_{SLICE_PROMPT}_slice_prompt.csv\")\n",
    "SAVE_SEG_DIR = os.path.join(BASE_DIR, f\"data_in_nii/SAM2_auto_seg_nii_{SEED_COUNT}shot_2class_{SLICE_PROMPT}_slice_prompt\")\n",
    "os.makedirs(SAVE_SEG_DIR, exist_ok=True)\n",
    "\n",
    "print(json.dumps({\n",
    "    \"dataset\": ACTIVE_DATASET,\n",
    "    \"stage\": ACTIVE_STAGE,\n",
    "    \"seed_count\": SEED_COUNT,\n",
    "    \"slice_prompt\": SLICE_PROMPT,\n",
    "    \"result_root\": RESULT_ROOT,\n",
    "    \"merged_csv\": MERGED_CSV,\n",
    "    \"save_seg_dir\": SAVE_SEG_DIR\n",
    "}, indent=2))\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Utility Functions\n",
    "# ==============================================================\n",
    "\n",
    "def keep_largest_connected_component(mask):\n",
    "    mask = mask.astype(np.uint8)\n",
    "    if mask.ndim == 3:\n",
    "        structure = np.ones((3,3,3), np.uint8)\n",
    "        labeled, num = label(mask, structure=structure)\n",
    "        if num == 0:\n",
    "            return mask\n",
    "        counts = np.bincount(labeled.ravel())\n",
    "        counts[0] = 0\n",
    "        return (labeled == np.argmax(counts)).astype(np.uint8)\n",
    "    elif mask.ndim == 2:\n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "        if num_labels <= 1: return mask\n",
    "        largest = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "        return (labels == largest).astype(np.uint8)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported mask dimension\")\n",
    "\n",
    "def save_array_to_nii(arr, ref_path, save_path):\n",
    "    ref = sitk.ReadImage(ref_path)\n",
    "    out = sitk.GetImageFromArray(arr)\n",
    "    out.CopyInformation(ref)\n",
    "    sitk.WriteImage(out, save_path)\n",
    "    print(f\"ðŸ’¾ Saved NIfTI: {save_path}\")\n",
    "\n",
    "def dice_binary(m1, m2):\n",
    "    m1, m2 = (m1>0), (m2>0)\n",
    "    inter = np.logical_and(m1,m2).sum()\n",
    "    union = m1.sum() + m2.sum()\n",
    "    return 1.0 if union==0 else 2*inter/union\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Step 1: Merge IOU & Mapping\n",
    "# ==============================================================\n",
    "\n",
    "all_records = []\n",
    "for pid_folder in sorted(os.listdir(RESULT_ROOT)):\n",
    "    iou_csv = os.path.join(RESULT_ROOT, pid_folder, f\"{pid_folder}_iou_results.csv\")\n",
    "    map_csv = os.path.join(IMG_MAPPING_ROOT, pid_folder, f\"{pid_folder}_mapping.csv\")\n",
    "    if not (os.path.exists(iou_csv) and os.path.exists(map_csv)):\n",
    "        continue\n",
    "    df_iou = pd.read_csv(iou_csv)\n",
    "    df_map = pd.read_csv(map_csv)\n",
    "    df = pd.merge(df_map, df_iou, on=\"frame_idx\", how=\"left\")\n",
    "    df[\"pid_folder\"] = pid_folder\n",
    "    all_records.append(df)\n",
    "\n",
    "df_all = pd.concat(all_records, ignore_index=True)\n",
    "df_all.to_csv(MERGED_CSV, index=False)\n",
    "print(f\"ðŸ“„ Merged CSV saved: {MERGED_CSV}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Step 2: Apply Refinement Filtering\n",
    "# ==============================================================\n",
    "\n",
    "# Example: keep only top X% by IoU\n",
    "if R[\"iou_dataset_top\"]:\n",
    "    cutoff = df_all[\"iou_sum\"].quantile(1 - R[\"iou_dataset_top\"])\n",
    "    df_all = df_all[df_all[\"iou_sum\"] >= cutoff]\n",
    "\n",
    "if R[\"iou_slice_top\"]:\n",
    "    df_all = df_all.groupby(\"slice_id\").apply(\n",
    "        lambda x: x.nlargest(max(1, int(len(x)*R[\"iou_slice_top\"])), \"iou_sum\")\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "# Dice & area constraints would be applied here in Stage2/Stage3\n",
    "# (needs SAM2 preds vs nnUNet preds; left as placeholder)\n",
    "# if R[\"dice_thresh\"]: ...\n",
    "# if R[\"area_ratio\"]: ...\n",
    "\n",
    "print(f\"âœ… Filtered data count after {ACTIVE_STAGE}: {len(df_all)}\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Step 3: Reconstruct NIfTI per PID\n",
    "# ==============================================================\n",
    "\n",
    "def process_pid(item):\n",
    "    pid, group = item\n",
    "    save_path = os.path.join(SAVE_SEG_DIR, f\"{pid}_sam2_seg_2class.nii.gz\")\n",
    "    if os.path.exists(save_path): return\n",
    "    group = group.sort_values(\"slice_id\")\n",
    "    img_paths = glob.glob(os.path.join(BASE_DIR, \"data_in_nii/img_in_nii_L4L5\", f\"{pid}*.nii.gz\"))\n",
    "    if not img_paths: return\n",
    "    ref_img = sitk.ReadImage(img_paths[0])\n",
    "    H, W = sitk.GetArrayFromImage(ref_img).shape[1:]\n",
    "    seg_slices = []\n",
    "    for _, r in group.iterrows():\n",
    "        frame_idx = r[\"frame_idx\"]\n",
    "        pred_dir = os.path.join(RESULT_ROOT, pid, \"SAM2_seg_mask_nolap\")\n",
    "        slice_mask = np.zeros((H,W), np.uint8)\n",
    "        for obj in [1,2]:\n",
    "            png = os.path.join(pred_dir, f\"frame_{frame_idx}_obj_{obj}.png\")\n",
    "            if not os.path.exists(png): continue\n",
    "            m = cv2.imread(png, cv2.IMREAD_GRAYSCALE)\n",
    "            if m is None or m.max()==0: continue\n",
    "            m_bin = (m==np.unique(m)[1]).astype(np.uint8)\n",
    "            m_bin = keep_largest_connected_component(m_bin)\n",
    "            m_bin = cv2.resize(m_bin, (W,H), interpolation=cv2.INTER_NEAREST)\n",
    "            slice_mask[m_bin>0] = obj\n",
    "        seg_slices.append(slice_mask)\n",
    "    if not seg_slices: return\n",
    "    seg_stack = np.stack(seg_slices)\n",
    "    save_array_to_nii(seg_stack, img_paths[0], save_path)\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=8) as exe:\n",
    "    list(tqdm(exe.map(process_pid, df_all.groupby(\"pid\")), total=df_all[\"pid\"].nunique()))\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Step 4: Evaluate DSC\n",
    "# ==============================================================\n",
    "\n",
    "label_dir = os.path.join(BASE_DIR, P[\"label_dir_2c\"])\n",
    "results = []\n",
    "for pred_path in tqdm(glob.glob(os.path.join(SAVE_SEG_DIR, \"*.nii.gz\")), desc=\"Eval\"):\n",
    "    pid = os.path.basename(pred_path).split(\"_sam2_seg_2class\")[0]\n",
    "    gt_paths = glob.glob(os.path.join(label_dir, f\"{pid}*.nii.gz\"))\n",
    "    if not gt_paths: continue\n",
    "    pred = sitk.GetArrayFromImage(sitk.ReadImage(pred_path))\n",
    "    gt = sitk.GetArrayFromImage(sitk.ReadImage(gt_paths[0]))\n",
    "    results.append([pid, dice_binary(pred==1, gt==1), dice_binary(pred==2, gt==2)])\n",
    "\n",
    "df_dsc = pd.DataFrame(results, columns=[\"pid\", \"dsc_class1\", \"dsc_class2\"])\n",
    "df_dsc.to_csv(os.path.join(BASE_DIR, f\"{ACTIVE_DATASET}_SAM2_auto_seg_DSC_summary_{SEED_COUNT}seed_2class.csv\"), index=False)\n",
    "\n",
    "print(\"âœ… DSC summary saved\")\n",
    "print(df_dsc.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9e531",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d63e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daac0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a19c42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a512f489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d4b898-ef5f-40ba-bf43-1de88ed4f2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
