{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e472b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Setup & Initialization ========================\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Configure device\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------ Load SAM2 Predictor --------------------------\n",
    "# Import SAM2 video predictor builder\n",
    "\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "# NOTE: You must configure the following paths manually before running.\n",
    "# - sam2_checkpoint: Path to the SAM2 model checkpoint (.pt file).\n",
    "#   Download from the official release and place it under \"checkpoints/\" or another folder.\n",
    "# - model_cfg: Path to the corresponding model config (.yaml file).\n",
    "#   Ensure this matches the checkpoint version.\n",
    "sam2_checkpoint = \"checkpoints/sam2.1_hiera_large.pt\"   # <-- Change if your path differs\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"        # <-- Change if your path differs\n",
    "\n",
    "# Build predictor instance with the specified configuration and checkpoint\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ddbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Utility Functions ========================\n",
    "\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    \"\"\"Overlay mask on an image plot.\"\"\"\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def process_frame(args):\n",
    "    \"\"\"Save segmentation masks and visualization for a single frame (multiprocessing-safe).\"\"\"\n",
    "    key, idx, video_segments_todo, video_dir, frame_names, prompt_dir = args\n",
    "    seg_dir = os.path.join(prompt_dir, f\"SAM2_seg_mask_{key}\")\n",
    "    vis_dir = os.path.join(prompt_dir, f\"SAM2_seg_plot_{key}\")\n",
    "\n",
    "    frame_path = os.path.join(video_dir, frame_names[idx])\n",
    "    frame_image = Image.open(frame_path)\n",
    "    combined_mask = None\n",
    "\n",
    "    for obj_id, mask in video_segments_todo[idx].items():\n",
    "        mask = np.squeeze(mask) if mask.ndim > 2 else mask\n",
    "        scaled_mask = mask * (obj_id * 50)\n",
    "\n",
    "        Image.fromarray(scaled_mask.astype(\"uint8\")).save(\n",
    "            os.path.join(seg_dir, f\"frame_{idx}_obj_{obj_id}.png\"), \"PNG\")\n",
    "\n",
    "        combined_mask = np.maximum(combined_mask, scaled_mask) if combined_mask is not None else scaled_mask\n",
    "\n",
    "    if combined_mask is not None:\n",
    "        Image.fromarray(combined_mask.astype(\"uint8\")).save(\n",
    "            os.path.join(seg_dir, f\"frame_{idx}_combined.png\"), \"PNG\")\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"Frame {idx} ({key})\")\n",
    "    plt.imshow(frame_image, cmap=\"gray\")\n",
    "    for obj_id, mask in video_segments_todo[idx].items():\n",
    "        show_mask(np.squeeze(mask) if mask.ndim > 2 else mask, plt.gca(), obj_id=obj_id)\n",
    "    plt.savefig(os.path.join(vis_dir, f\"frame_{idx}_visualization.png\"), format=\"png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"ðŸ–¼ï¸ Processed frame {idx} for {key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd7086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================== Main PID Processing ========================\n",
    "def process_one_pid(pid, data_dir, seed_count):\n",
    "    print(f\"\\nðŸš€ Processing {pid} with {seed_count}-seed\")\n",
    "\n",
    "    # Directories\n",
    "    img_dir = os.path.join(data_dir, f\"img_in_jpg_to_sam2_{seed_count}seed\", pid)\n",
    "    label_dir = os.path.join(data_dir, f\"label_in_png_to_sam2_{seed_count}seed\", pid)\n",
    "    prompt_dir = os.path.join(data_dir, f\"sam2_results_by_pid_{seed_count}seed\", pid)    \n",
    "    csv_map_path = os.path.join(img_dir, f\"{pid}_mapping.csv\")\n",
    "\n",
    "    # Skip if results already exist\n",
    "    seg_mask_yeslap_dir = os.path.join(prompt_dir, \"SAM2_seg_plot_yeslap\")\n",
    "    if os.path.isdir(seg_mask_yeslap_dir) and len(os.listdir(seg_mask_yeslap_dir)) > 0:\n",
    "        print(f\"âœ… Results already exist for {pid}, skipping.\")\n",
    "        return\n",
    "    \n",
    "    os.makedirs(prompt_dir, exist_ok=True)\n",
    "\n",
    "    # Load mapping\n",
    "    df_map = pd.read_csv(csv_map_path)\n",
    "    seed_rows = df_map[df_map[\"category\"] == \"seed\"]\n",
    "    prompt_idx = sorted(seed_rows[\"frame_idx\"].tolist())\n",
    "\n",
    "    # Prepare frames\n",
    "    frame_names = sorted([p for p in os.listdir(img_dir) if p.lower().endswith(\".jpg\")],\n",
    "                         key=lambda p: int(os.path.splitext(p)[0]))\n",
    "\n",
    "    # Initialize predictor state\n",
    "    inference_state = predictor.init_state(video_path=img_dir)\n",
    "    predictor.reset_state(inference_state)\n",
    "\n",
    "    # Class ranges (example thresholds)\n",
    "    class_ranges = {1: (80, 120), 2: (180, 220)}\n",
    "\n",
    "    # Add prompt masks\n",
    "    for idx in prompt_idx:\n",
    "        img_path = os.path.join(img_dir, frame_names[idx])\n",
    "        seg_path = os.path.join(label_dir, str(idx).zfill(5) + \".png\")\n",
    "\n",
    "        try:\n",
    "            seg_array = np.array(Image.open(seg_path).convert(\"L\"))\n",
    "            img_array = np.array(Image.open(img_path).convert(\"L\"))\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading image {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "        class_masks = {}\n",
    "        for class_id, (low, high) in class_ranges.items():\n",
    "            mask = (seg_array >= low) & (seg_array <= high)\n",
    "            class_mask = np.zeros_like(seg_array, dtype=np.uint8)\n",
    "            class_mask[mask] = 1\n",
    "            class_masks[class_id] = np.array(\n",
    "                Image.fromarray(class_mask).resize(img_array.shape[::-1], Image.NEAREST))\n",
    "\n",
    "        for ann_obj_id, class_mask in class_masks.items():\n",
    "            try:\n",
    "                predictor.add_new_mask(inference_state, frame_idx=idx, obj_id=ann_obj_id, mask=class_mask)\n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Error in add_new_mask: {e}\")\n",
    "\n",
    "        print(f\"âœ… Added prompts for frame {idx} ({frame_names[idx]})\")\n",
    "        \n",
    "# ======================== Inference & IOU Saving ========================\n",
    "    video_segments, video_segments_nolap, iou_score = {}, {}, {}\n",
    "\n",
    "    for out in predictor.propagate_in_video(\n",
    "        inference_state, iou_score_return=True, max_frame_num_to_track=len(frame_names)\n",
    "    ):\n",
    "        out_frame_idx, out_obj_ids, out_mask_logits = out[:3]\n",
    "        ious = out[3]\n",
    "\n",
    "        # Overlapping masks\n",
    "        video_segments[out_frame_idx] = {\n",
    "            obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "            for i, obj_id in enumerate(out_obj_ids)}\n",
    "\n",
    "        # IOU scores\n",
    "        iou_score[out_frame_idx] = {obj_id: ious[i] for i, obj_id in enumerate(out_obj_ids)}\n",
    "\n",
    "        # Non-overlapping masks\n",
    "        frame_masks = {\n",
    "            obj_id: (out_mask_logits[i] > 0.0).cpu().numpy() * out_mask_logits[i].cpu().numpy()\n",
    "            for i, obj_id in enumerate(out_obj_ids)}\n",
    "        stacked_masks = np.stack([frame_masks[obj_id] > 0 for obj_id in frame_masks], axis=0)\n",
    "        stacked_logits = np.stack([frame_masks[obj_id] for obj_id in frame_masks], axis=0)\n",
    "        max_logits_indices = np.argmax(stacked_logits, axis=0)\n",
    "\n",
    "        video_segments_nolap[out_frame_idx] = {\n",
    "            obj_id: (max_logits_indices == i).astype(np.uint8) & stacked_masks[i]\n",
    "            for i, obj_id in enumerate(frame_masks)}\n",
    "\n",
    "    # Save IOU results\n",
    "    iou_csv_path = os.path.join(prompt_dir, pid + \"_iou_results.csv\")\n",
    "    results = []\n",
    "    for f_idx in range(len(frame_names)):\n",
    "        for o_id, iou in iou_score[f_idx].items():\n",
    "            max_iou = torch.max(iou).item()\n",
    "            results.append({\"frame_idx\": f_idx, \"obj_id\": o_id, \"iou\": max_iou})\n",
    "    results_df = pd.DataFrame(results)\n",
    "    wide_df = results_df.pivot(index=\"frame_idx\", columns=\"obj_id\", values=\"iou\")\n",
    "    wide_df.columns = [f\"iou_id{col}\" for col in wide_df.columns]\n",
    "    wide_df[\"iou_sum\"] = wide_df.sum(axis=1)\n",
    "    wide_df.to_csv(iou_csv_path, index=True)\n",
    "    print(f\"ðŸ“„ Saved IOU results to {iou_csv_path}\")\n",
    "\n",
    "# ======================== Save Masks & Visualizations ========================\n",
    "    video_segments_dict = {\"yeslap\": video_segments, \"nolap\": video_segments_nolap}\n",
    "    for key in video_segments_dict:\n",
    "        os.makedirs(os.path.join(prompt_dir, f\"SAM2_seg_mask_{key}\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(prompt_dir, f\"SAM2_seg_plot_{key}\"), exist_ok=True)\n",
    "\n",
    "    tasks = []\n",
    "    for key, video_segments_todo in video_segments_dict.items():\n",
    "        for idx in range(len(frame_names)):\n",
    "            tasks.append((key, idx, video_segments_todo, img_dir, frame_names, prompt_dir))\n",
    "\n",
    "    with Pool(min(2, cpu_count())) as pool:\n",
    "        pool.map(process_frame, tasks)\n",
    "\n",
    "    print(f\"ðŸŽ‰ Finished processing {pid}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bfa81",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Batch Processing of All PIDs\n",
    "# ================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "def sort_key(pid):\n",
    "    \"\"\"\n",
    "    Sort PIDs numerically if possible, otherwise lexicographically.\n",
    "    Example: pid_2 < pid_10\n",
    "    \"\"\"\n",
    "    m = re.search(r'\\d+', pid.split('_')[0])\n",
    "    return (False, int(m.group()), pid.lower()) if m else (True, 0, pid.lower())\n",
    "\n",
    "# --- Configuration ---\n",
    "dataset = \"CT_word\"  # or \"CT_TT\"\n",
    "data_dir = os.path.join(\"notebooks\", \"videos\", dataset, \"data_in_jpg\")\n",
    "seed_counts = [1]  # list of seed numbers to run, e.g. [1, 3, 5]\n",
    "\n",
    "# --- Collect all PIDs from reference (1-seed folder) ---\n",
    "ref_dir = os.path.join(data_dir, \"img_in_jpg_to_sam2_1seed\")\n",
    "pids = sorted(\n",
    "    [f for f in os.listdir(ref_dir) if os.path.isdir(os.path.join(ref_dir, f))],\n",
    "    key=sort_key\n",
    ")\n",
    "\n",
    "print(f\"ðŸ” Found {len(pids)} PIDs in dataset: {dataset}\")\n",
    "\n",
    "# --- Process each PID across all seeds ---\n",
    "for pid in tqdm(pids, desc=\"Processing PIDs\"):\n",
    "    for seed in seed_counts:\n",
    "        pid_dir = os.path.join(data_dir, f\"img_in_jpg_to_sam2_{seed}seed\", pid)\n",
    "        if os.path.exists(pid_dir):\n",
    "            try:\n",
    "                process_one_pid(pid, data_dir, seed)\n",
    "            except Exception as e:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a067d-3ce8-4a45-98cf-cdd58d873afc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
